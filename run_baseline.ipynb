{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import NHITS\n",
    "from neuralforecast.losses.pytorch import HuberLoss\n",
    "from neuralforecast.losses.numpy import mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y_hat_df, av_mask, model_name):\n",
    "    results_df = Y_hat_df.copy()\n",
    "    # Filter values with at least 1 available mask in input window\n",
    "    results_df = results_df.merge(av_mask[['unique_id', 'cutoff', 'sum_av_mask']], on=['unique_id', 'cutoff'], how='left')\n",
    "    results_df = results_df[results_df['sum_av_mask'] > 0].reset_index(drop=True)\n",
    "    # Filter ffill values of y\n",
    "    results_df = results_df[results_df.available_mask==1]\n",
    "    # Keep critical values of y\n",
    "    results_critical_df = results_df[(results_df.y<=70) | (results_df.y>=180)]\n",
    "    return mae(results_df['y'], results_df[model_name]), mae(results_critical_df['y'], results_critical_df[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>available_mask</th>\n",
       "      <th>CHO</th>\n",
       "      <th>insulin</th>\n",
       "      <th>sum_av_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#559</td>\n",
       "      <td>2021-12-07 01:20:00</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#559</td>\n",
       "      <td>2021-12-07 01:25:00</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#559</td>\n",
       "      <td>2021-12-07 01:30:00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#559</td>\n",
       "      <td>2021-12-07 01:35:00</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#559</td>\n",
       "      <td>2021-12-07 01:40:00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id              cutoff      y  available_mask  CHO  insulin  \\\n",
       "0      #559 2021-12-07 01:20:00  101.0               1  0.0      0.0   \n",
       "1      #559 2021-12-07 01:25:00   98.0               1  0.0      0.0   \n",
       "2      #559 2021-12-07 01:30:00  104.0               1  0.0      0.0   \n",
       "3      #559 2021-12-07 01:35:00  112.0               1  0.0      0.0   \n",
       "4      #559 2021-12-07 01:40:00  120.0               1  0.0      0.0   \n",
       "\n",
       "   sum_av_mask  \n",
       "0          1.0  \n",
       "1          2.0  \n",
       "2          3.0  \n",
       "3          4.0  \n",
       "4          5.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/scratch/wpotosna/data/ohiot1dm_exog_9_day_test.csv')\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "df = []\n",
    "unique_ids = data['unique_id'].unique()\n",
    "for unique_id in unique_ids:\n",
    "    df_uid = data[data['unique_id'] == unique_id].reset_index(drop=True)\n",
    "    df_uid[\"sum_av_mask\"] = df_uid['available_mask'].rolling(window=120, min_periods=1).sum()\n",
    "    df.append(df_uid)\n",
    "av_mask = pd.concat(df).reset_index(drop=True)\n",
    "av_mask = av_mask.rename(columns={'ds': 'cutoff'})\n",
    "av_mask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results_glucose/h_6/baselines/nhits_20230829.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Y_hat_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults_glucose/h_6/baselines/nhits_20230829.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m Y_hat_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(Y_hat_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m Y_hat_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcutoff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(Y_hat_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcutoff\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/neuralforecast/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neuralforecast/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/neuralforecast/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neuralforecast/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/neuralforecast/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results_glucose/h_6/baselines/nhits_20230829.csv'"
     ]
    }
   ],
   "source": [
    "Y_hat_df = pd.read_csv('results_glucose/h_6/baselines/nhits_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "\n",
    "print('NHITS baseline ALL: ', mae_all)\n",
    "print('NHITS baseline CRITICAL: ', mae_critical)\n",
    "\n",
    "Y_hat_df = pd.read_csv('results_glucose/h_6/baselines/tft_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoTFT')\n",
    "\n",
    "print('TFT baseline ALL: ', mae_all)\n",
    "print('TFT baseline CRITICAL: ', mae_critical)\n",
    "\n",
    "Y_hat_df = pd.read_csv('results_glucose/h_6/exogenous/nhits_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "print('NHITS exog ALL: ', mae_all)\n",
    "print('NHITS exog CRITICAL: ', mae_critical)\n",
    "\n",
    "Y_hat_df = pd.read_csv('results_glucose/h_6/treat/nhits_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS_TREAT')\n",
    "print('NHITS TREAT ALL: ', mae_all)\n",
    "print('NHITS TREAT CRITICAL: ', mae_critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS baseline ALL:  9.319583626742984\n",
      "NHITS baseline CRITICAL:  10.50554280553876\n",
      "TFT baseline ALL:  9.259035800329098\n",
      "TFT baseline CRITICAL:  10.371936204513847\n",
      "TFT exog ALL:  9.464974399592458\n",
      "TFT exog CRITICAL:  11.417223404672573\n",
      "NHITS exog ALL:  8.972649823354212\n",
      "NHITS exog CRITICAL:  10.100923448845085\n",
      "NHITS TREAT ALL:  8.94415280577869\n",
      "NHITS TREAT CRITICAL:  10.288121691332083\n",
      "NHITS TREAT ALL:  8.902128148961392\n",
      "NHITS TREAT CRITICAL:  9.885354577064941\n"
     ]
    }
   ],
   "source": [
    "Y_hat_df = pd.read_csv('results_glucose/h_6/baselines/nhits_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "\n",
    "print('NHITS baseline ALL: ', mae_all)\n",
    "print('NHITS baseline CRITICAL: ', mae_critical)\n",
    "\n",
    "Y_hat_df = pd.read_csv('results_glucose/h_6/baselines/tft_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoTFT')\n",
    "\n",
    "print('TFT baseline ALL: ', mae_all)\n",
    "print('TFT baseline CRITICAL: ', mae_critical)\n",
    "\n",
    "Y_hat_df = pd.read_csv('results_glucose/h_6/exogenous/tft_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoTFT')\n",
    "\n",
    "print('TFT exog ALL: ', mae_all)\n",
    "print('TFT exog CRITICAL: ', mae_critical)\n",
    "\n",
    "Y_hat_df = pd.read_csv('results_glucose/h_6/exogenous/nhits_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "print('NHITS exog ALL: ', mae_all)\n",
    "print('NHITS exog CRITICAL: ', mae_critical)\n",
    "\n",
    "Y_hat_df = pd.read_csv('results_glucose/h_6/treat/nhits_20230829.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS_TREAT')\n",
    "print('NHITS TREAT ALL: ', mae_all)\n",
    "print('NHITS TREAT CRITICAL: ', mae_critical)\n",
    "\n",
    "\n",
    "Y_hat_df = pd.read_csv('results_glucose/h_6/treat/nhits_20230829_cho.csv')\n",
    "Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "\n",
    "mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS_TREAT')\n",
    "print('NHITS TREAT ALL: ', mae_all)\n",
    "print('NHITS TREAT CRITICAL: ', mae_critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ids = ['nhits_20230829_cho', 'nhits_20230831_1', 'nhits_20230831_2','nhits_20230831_3',\n",
    "                  'nhits_20230831_4', 'nhits_20230831_5', 'nhits_20230831_6', 'nhits_20230831_7', 'nhits_20230831_8']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_6/treat/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS_TREAT')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [8.902128148961392, 8.898860100422286, 8.731333180774268, 8.908003669432745, 8.97602519511188, 8.953040576677637, 8.93725488928601, 8.69443896291447, 8.955262941965394]\n",
      "NHITS TREAT CRITICAL:  [9.885354577064941, 10.067645513756197, 9.938478664001089, 10.066430550051397, 10.141582577246343, 10.183065959562825, 10.163202115990446, 9.969622231980892, 10.262580547768776]\n",
      "NHITS TEST ALL AVERAGE:  8.88403862950512\n",
      "NHITS TEST CRITICAL AVERAGE:  10.07532919304699\n",
      "NHITS TEST ALL STD:  0.09516995759204688\n",
      "NHITS TEST CRITICAL STD:  0.11780966316668567\n"
     ]
    }
   ],
   "source": [
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOD ONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [9.294324594925511, 9.29302931093339]\n",
      "NHITS TREAT CRITICAL:  [10.46137866366852, 10.351821920168703]\n",
      "NHITS TEST ALL AVERAGE:  9.29367695292945\n",
      "NHITS TEST CRITICAL AVERAGE:  10.406600291918611\n",
      "NHITS TEST ALL STD:  0.0006476419960605995\n",
      "NHITS TEST CRITICAL STD:  0.05477837174990885\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = ['nhits_20230901_1', 'nhits_20230901_2']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_6/baselines/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)\n",
    "\n",
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [8.770153566167709, 8.825971526894087, 8.820030653921464, 8.929680516670206]\n",
      "NHITS TREAT CRITICAL:  [9.85622752483674, 9.965233632996737, 9.87923582234853, 10.091018492184665]\n",
      "NHITS TEST ALL AVERAGE:  8.836459065913367\n",
      "NHITS TEST CRITICAL AVERAGE:  9.947928868091667\n",
      "NHITS TEST ALL STD:  0.05802270971996216\n",
      "NHITS TEST CRITICAL STD:  0.09206243302743222\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = ['nhits_20230901_2_1', 'nhits_20230901_2_2', 'nhits_20230901_2_3', 'nhits_20230901_2_4']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_6/treat/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS_TREAT')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)\n",
    "\n",
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [24.448452659358573, 24.842071815832718]\n",
      "NHITS TREAT CRITICAL:  [30.777990970817207, 30.94805512464986]\n",
      "NHITS TEST ALL AVERAGE:  24.645262237595645\n",
      "NHITS TEST CRITICAL AVERAGE:  30.863023047733535\n",
      "NHITS TEST ALL STD:  0.19680957823707246\n",
      "NHITS TEST CRITICAL STD:  0.08503207691632575\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = ['nhits_20230901_1', 'nhits_20230901_2']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_24/baselines/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)\n",
    "\n",
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [23.331997588782052, 23.853627117784963]\n",
      "NHITS TREAT CRITICAL:  [28.555594490093043, 29.01315113623619]\n",
      "NHITS TEST ALL AVERAGE:  23.592812353283506\n",
      "NHITS TEST CRITICAL AVERAGE:  28.784372813164616\n",
      "NHITS TEST ALL STD:  0.2608147645014558\n",
      "NHITS TEST CRITICAL STD:  0.2287783230715732\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = ['nhits_20230901_1', 'nhits_20230901_2']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_24/exogenous/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)\n",
    "\n",
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [23.598199761300105, 23.904786123717106]\n",
      "NHITS TREAT CRITICAL:  [28.958337218544642, 29.10686700828887]\n",
      "NHITS TEST ALL AVERAGE:  23.751492942508605\n",
      "NHITS TEST CRITICAL AVERAGE:  29.032602113416758\n",
      "NHITS TEST ALL STD:  0.15329318120850033\n",
      "NHITS TEST CRITICAL STD:  0.07426489487211363\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = ['nhits_20230901_1', 'nhits_20230901_2']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_24/treat/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS_TREAT')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)\n",
    "\n",
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 days of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [9.551207755900776, 10.03209366250457]\n",
      "NHITS TREAT CRITICAL:  [10.763396118152134, 11.552775866126495]\n",
      "NHITS TEST ALL AVERAGE:  9.791650709202674\n",
      "NHITS TEST CRITICAL AVERAGE:  11.158085992139315\n",
      "NHITS TEST ALL STD:  0.2404429533018968\n",
      "NHITS TEST CRITICAL STD:  0.3946898739871809\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = ['nhits_20230901_10_1', 'nhits_20230901_10_2']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_6/baselines/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)\n",
    "\n",
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [9.203094116470272, 9.411317519309483]\n",
      "NHITS TREAT CRITICAL:  [10.356497463205951, 10.495450755653646]\n",
      "NHITS TEST ALL AVERAGE:  9.307205817889876\n",
      "NHITS TEST CRITICAL AVERAGE:  10.425974109429799\n",
      "NHITS TEST ALL STD:  0.10411170141960557\n",
      "NHITS TEST CRITICAL STD:  0.06947664622384764\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = ['nhits_20230901_10_1', 'nhits_20230901_10_2']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_6/exogenous/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)\n",
    "\n",
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS TREAT ALL:  [9.530075877390095, 9.149628339832736]\n",
      "NHITS TREAT CRITICAL:  [10.430569400123957, 10.283910443010038]\n",
      "NHITS TEST ALL AVERAGE:  9.339852108611415\n",
      "NHITS TEST CRITICAL AVERAGE:  10.357239921566997\n",
      "NHITS TEST ALL STD:  0.19022376877867941\n",
      "NHITS TEST CRITICAL STD:  0.07332947855695959\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = ['nhits_20230901_10_1', 'nhits_20230901_10_2']\n",
    "mae_alls=[]\n",
    "mae_criticals=[]\n",
    "for experiment_id in experiment_ids:\n",
    "    Y_hat_df = pd.read_csv(f'results_glucose/h_6/treat/{experiment_id}.csv')\n",
    "    Y_hat_df['ds'] = pd.to_datetime(Y_hat_df['ds'])\n",
    "    Y_hat_df['cutoff'] = pd.to_datetime(Y_hat_df['cutoff'])\n",
    "    mae_all, mae_critical = evaluate(Y_hat_df, av_mask, 'AutoNHITS_TREAT')\n",
    "    mae_alls.append(mae_all)\n",
    "    mae_criticals.append(mae_critical)\n",
    "\n",
    "print('NHITS TREAT ALL: ', mae_alls)\n",
    "print('NHITS TREAT CRITICAL: ', mae_criticals)\n",
    "print('NHITS TEST ALL AVERAGE: ', np.mean(mae_alls))\n",
    "print('NHITS TEST CRITICAL AVERAGE: ', np.mean(mae_criticals))\n",
    "print('NHITS TEST ALL STD: ', np.std(mae_alls))\n",
    "print('NHITS TEST CRITICAL STD: ', np.std(mae_criticals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job Title: Time Series Data Collector\n",
    "\n",
    "### Job Summary:\n",
    "The Time Series Data Collector will be responsible for identifying, scraping, and processing time series datasets from various online sources. This role will work closely with data scientists and machine learning engineers to provide high-quality data that will contribute to analytics and predictive modeling projects.\n",
    "\n",
    "### Responsibilities:\n",
    "- **Identify Sources**: Search for reliable data sources on the Internet that provide time series datasets relevant to the business objectives. An initial list of examples will be provided.\n",
    "\n",
    "   Preferable characteristics:\n",
    " * Have a reliable source. Some examples include: Kaggle competitions, public academic papers, official statistics or data published by governments, companies, or organizations.\n",
    " * Diversity in key characteristics including: sampling frequency, domain, size (number of series and observations).\n",
    " * Large volume in both number of time series and data points.\n",
    "\n",
    "- **Data Scraping**: Some datasets might requiring using tools like Python libraries (e.g., Beautiful Soup, Selenium, Scrapy) to scrape time series data from websites, APIs, and other platforms.\n",
    "\n",
    "- **Data Processing**: Clean and preprocess raw data to handle missing values and transform the data into a provided standardized format. Compute basic statistics for the dataset and create short reports describing the data and required information (eg. frequency, domain, number of series). \n",
    "  \n",
    "- **Data Storage**: Work with database administrators to store processed data in AWS S3. Each dataset should be stored separately in individual parquet files.\n",
    "  \n",
    "- **Documentation**: Maintain comprehensive documentation that describes data sources, data transformations, and any challenges encountered during the process.\n",
    "  \n",
    "- **Adherence to Legal Guidelines**: Ensure that data scraping and usage adhere to legal and ethical guidelines, including copyright laws and data protection regulations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identify Sources\n",
    "\n",
    "The first step of the process is to identify useful time series data sources. A time series is a sequence of observations recorded over time. We focus on time series with discrete and regular sampling intervals (eg. daily, monthly, weekly, data). A time series dataset will usually contain the following information:\n",
    "\n",
    "- Datestamps: when the value was recorded\n",
    "- Values: values of the time series\n",
    "- Identifier: some datasets contain several time series (eg. different products), and are identified with an id or name.\n",
    "\n",
    "Some example of domains of time series and examples we are interested are: \n",
    "\n",
    "1. Demand (eg. sales of a product, quantity sold)\n",
    "2. Weather data (eg. temperature, sensor readings, pollution)\n",
    "3. Finance data (eg. prices of a stock or asset)\n",
    "4. Medicine and epidemics (eg. ECG and sensor data, number of patients with a disease)\n",
    "5. Macro-economics and demographics (eg. population, PIB, indicators)\n",
    "6. Sensor data and IoT (eg. usage of a server, sensors readings on machines, usually high frequency)\n",
    "7. Electricity (eg. demand of a market or house, prices, output of wind turbines)\n",
    "8. Transportation (eg. number of airline passengers, traffic at intersections)\n",
    "9. Web traffic (eg. traffic on a website, Google trends)\n",
    "\n",
    "Avoid time series from the following domains: audio data, text (parsed), videos or images (parsed), and occurances of sparse events.\n",
    "\n",
    "Some potential sources are:\n",
    "\n",
    "1. Kaggle competitions (focus mostly on time series forecasting and anomaly detection)\n",
    "2. Academic papers (focus mostly on time series forecasting and anomaly detection)\n",
    "3. Online repositories\n",
    "4. Public data from companies and government\n",
    "\n",
    "Sources 1-3 will probably require minimal effort to obtain as they are usually already preprocessed and placed in files. On the contrary, datasets published by organizations might require scrapping or developing scripts to parse the data (eg: FRED https://fred.stlouisfed.org/).\n",
    "\n",
    "Finally, prioritize first:\n",
    "- Larger datasets (both number of time series and datestamps).\n",
    "- High-frequency data (eg. minutely, second data, or lower).\n",
    "- Sensor data\n",
    "\n",
    "Time series datasets we have already parsed:\n",
    "1. M-competitions (1,2,3,4,5)\n",
    "2. Monash repository and all its datasets (https://zenodo.org/communities/forecasting/?page=1&size=20)\n",
    "3. Wikipedia traffic\n",
    "\n",
    "## 2. Data Processing\n",
    "\n",
    "After downloading the data, parse it into the following standardized format. Each dataset should be contained in an individual parquet file, and should contain only the following four columns:\n",
    "- `unique_id`: identifier of each time series. If not provided in raw data, defined it as `unique_id_1`, `unique_id_2`, etc.\n",
    "- `ds`: column with the datestamp in Pandas format (no UTC).\n",
    "- `y`: value of the time series. If the original dataset contains multiple values per datestamp (commonly knows as a multivariate time series), split them in separate series with differenct values for `unique_id`.\n",
    "- `available_mask`: column with 0 or 1 indicating the time series has a value for the particular `ds`. This is used to identify missing values as 0. Do not impute missing data or fill the missing values with zeros.\n",
    "\n",
    "The following link contains examples of parsed data (without `available_mask`): https://nixtla.github.io/neuralforecast/examples/data_format.html.\n",
    "\n",
    "## 3. Data Storage and Documentation\n",
    "\n",
    "Finally, upload each dataset in a separate folder to S3 (bucket will be provided later). Each folder should contain:\n",
    "    1. Parquet with the parsed dataset\n",
    "    2. Markdown or text file with documentation of the dataset, including: source and general comments or challenges encountered.\n",
    "    3. Python script parsing the original data to the required format (use a separate script for each dataset).\n",
    "    4. If applicable, auxiliary scripts used to download the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
