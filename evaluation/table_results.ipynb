{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c687876-8698-4dae-92e7-8ecf69dba986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.losses.numpy import mape, mae\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3affa0-47ba-42aa-8fee-42b5cc2a81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_names(eval_df, experiment_name):\n",
    "    if experiment_name == 'size_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='tiny').columns.values,\n",
    "            eval_df.filter(regex='mini').columns.values,\n",
    "            eval_df.filter(regex='small').columns.values,\n",
    "            eval_df.filter(regex='base').columns.values,\n",
    "                           ])\n",
    "\n",
    "    elif experiment_name == 'tokenlen_ablation':\n",
    "        names = np.concatenate([eval_df.filter(regex='len8').columns.values,\n",
    "            eval_df.filter(regex='len16').columns.values,\n",
    "            eval_df.filter(regex='len32').columns.values,\n",
    "            eval_df.filter(regex='len64').columns.values,\n",
    "            eval_df.filter(regex='len96').columns.values,\n",
    "            eval_df.filter(regex='len128').columns.values,\n",
    "                           ])\n",
    "\n",
    "    elif experiment_name == 'pe_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='_relative').columns.values,\n",
    "            eval_df.filter(regex='_sincos').columns.values,\n",
    "            eval_df.filter(regex='rope').columns.values,\n",
    "                           ])\n",
    "\n",
    "    elif experiment_name == 'proj_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='linear').columns.values,\n",
    "            eval_df.filter(regex='residual').columns.values,\n",
    "                           ])\n",
    "\n",
    "    elif experiment_name == 'scaler_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='standard').columns.values,\n",
    "            eval_df.filter(regex='revin').columns.values,\n",
    "            eval_df.filter(regex='robust').columns.values,\n",
    "                           ])\n",
    "\n",
    "    elif experiment_name == 'loss_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='mae').columns.values,\n",
    "            eval_df.filter(regex='mse').columns.values,\n",
    "            eval_df.filter(regex='huber').columns.values,\n",
    "            eval_df.filter(regex='studentt').columns.values,\n",
    "                           ])\n",
    "        names = names[:4]\n",
    "\n",
    "    elif experiment_name == 'attn_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='encoderb').columns.values,\n",
    "            eval_df.filter(regex='encoderc').columns.values,\n",
    "            eval_df.filter(regex='decoder').columns.values,\n",
    "            eval_df.filter(regex='encoderdecoder').columns.values,\n",
    "                                ])\n",
    "\n",
    "    elif experiment_name == 'tokenization_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='none').columns.values,\n",
    "            eval_df.filter(regex='patch').columns.values,\n",
    "            eval_df.filter(regex='lag').columns.values,\n",
    "            eval_df.filter(regex='bin').columns.values,\n",
    "                                ])\n",
    "        names = names[:3]\n",
    "\n",
    "    elif experiment_name == 'contextlen_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='256').columns.values,\n",
    "            eval_df.filter(regex='512').columns.values,\n",
    "                                ])\n",
    "        names = names[:3]\n",
    "\n",
    "    elif experiment_name == 'decomp_ablation':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='tsdecomp').columns.values,\n",
    "            eval_df.filter(regex='fourierdecomp').columns.values,\n",
    "                                ])\n",
    "        names = names[:3]\n",
    "\n",
    "    elif experiment_name == 'nont5models':\n",
    "        names = np.concatenate([\n",
    "            eval_df.filter(regex='dlinear').columns.values,\n",
    "            eval_df.filter(regex='nlinear').columns.values,\n",
    "            eval_df.filter(regex='mlp').columns.values,\n",
    "            eval_df.filter(regex='nhits').columns.values,\n",
    "            eval_df.filter(regex='nbeats').columns.values,\n",
    "            eval_df.filter(regex='tsmixer').columns.values,\n",
    "            eval_df.filter(regex='lstm').columns.values,\n",
    "            eval_df.filter(regex='tcn').columns.values,\n",
    "            eval_df.filter(regex='timesnet').columns.values,\n",
    "            eval_df.filter(regex='tcn').columns.values,\n",
    "            eval_df.filter(regex='vanillatransformer').columns.values,\n",
    "            eval_df.filter(regex='itransformer').columns.values,\n",
    "            eval_df.filter(regex='autoformer').columns.values,\n",
    "            eval_df.filter(regex='informer').columns.values,\n",
    "            eval_df.filter(regex='tft').columns.values,\n",
    "            eval_df.filter(regex='patchtst').columns.values,\n",
    "                           ])\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf46364-b9ed-41b0-88e6-fe68e10e652e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eadbff-7932-4bc9-82ee-2e83fae66b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(dataset_name, n_compositions, experiment_name, experiment_mode):\n",
    "    \n",
    "    save_dir = 'ADD HERE'\n",
    "    n_series = 100\n",
    "    ood_ratio = 0.0\n",
    "    \n",
    "    save_path1 = f'{save_dir}/results_randomseed1/{experiment_name}/{dataset_name}_{n_compositions}bases/{experiment_mode}/{n_series}_samples/{experiment_mode}_model_aggregate_dataset_fcsts.csv'\n",
    "    eval_df1 = pd.read_csv(save_path1)\n",
    "    eval_df1.set_index(['unique_id', 'ds'], inplace=True)\n",
    "    \n",
    "    save_path2 = f'{save_dir}/results_randomseed5/{experiment_name}/{dataset_name}_{n_compositions}bases/{experiment_mode}/{n_series}_samples/{experiment_mode}_model_aggregate_dataset_fcsts.csv'\n",
    "    eval_df2 = pd.read_csv(save_path2)\n",
    "    eval_df2.set_index(['unique_id', 'ds'], inplace=True)\n",
    "    \n",
    "    save_path3 = f'{save_dir}/results_randomseed10/{experiment_name}/{dataset_name}_{n_compositions}bases/{experiment_mode}/{n_series}_samples/{experiment_mode}_model_aggregate_dataset_fcsts.csv'\n",
    "    eval_df3 = pd.read_csv(save_path3)\n",
    "    eval_df3.set_index(['unique_id', 'ds'], inplace=True)\n",
    "    \n",
    "    dict1 = {}\n",
    "    dict2 = {}\n",
    "    dict3 = {}\n",
    "    names = sort_names(eval_df1, experiment_name)\n",
    "    for col in names:\n",
    "        dict1[col] = mae(eval_df1[col], eval_df1['y'])\n",
    "        dict2[col] = mae(eval_df2[col], eval_df2['y'])\n",
    "        dict3[col] = mae(eval_df3[col], eval_df3['y'])\n",
    "    dicts = [dict1, dict2, dict3]\n",
    "\n",
    "    mean_result = {}\n",
    "    stdev_result = {}\n",
    "    for key in names:\n",
    "        values = [d[key] for d in dicts]\n",
    "        mean_result[key] = round(mean(values), 3)\n",
    "        stdev_result[key] = round(stdev(values), 3)\n",
    "\n",
    "    return mean_result, stdev_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c02017-1353-4db2-9b1d-55947e4a4059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202970ff-7f08-4e73-a732-399a830a5a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e06b3796-41e4-4dc0-adc7-15fe31338d7b",
   "metadata": {},
   "source": [
    "## Non-T5 Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3a131-9985-41ca-9f8c-0336c6c4d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['synthetic_sinusoid_composition',\n",
    "           'ecl_100_series',\n",
    "           'ettm2_100_series',\n",
    "           'solar1h_100_series',\n",
    "           'subseasonal_100_series',\n",
    "           'loopseattle_100_series'\n",
    "           ]\n",
    "\n",
    "n_comps = [2, 100, 100, 100, 100, 100]\n",
    "\n",
    "baseline_results = pd.DataFrame()\n",
    "for dataset_name, n_compositions in zip(dataset_names, n_comps):\n",
    "    mean_result, stdev_result = get_results(\n",
    "        dataset_name,\n",
    "        n_compositions,\n",
    "        'baselines',\n",
    "        experiment_mode='component')\n",
    "    col1 = pd.DataFrame([mean_result]).T\n",
    "    #col1 = pd.DataFrame([stdev_result]).T\n",
    "    \n",
    "    mean_result, stdev_result = get_results(\n",
    "        dataset_name,\n",
    "        n_compositions,\n",
    "        'baselines',\n",
    "        experiment_mode='aggregate')\n",
    "    col2 = pd.DataFrame([mean_result]).T\n",
    "    #col2 = pd.DataFrame([stdev_result]).T\n",
    "\n",
    "    baseline_results = pd.concat([baseline_results, col1, col2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea959d-df22-4356-9000-c71496bb65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00fddc-f355-4340-ae33-092d704504cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c6297d5-cd5d-4292-b27f-50461c50556f",
   "metadata": {},
   "source": [
    "## Ablation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc86a3-9ae7-4192-81c2-64a6bfc8e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'loopseattle_100_series'\n",
    "n_compositions = 100\n",
    "experiment_mode = 'aggregate' #'component' #\n",
    "\n",
    "mean_result, stdev_result = get_results(\n",
    "    dataset_name,\n",
    "    n_compositions,\n",
    "    'tokenization_ablation', # Replace with ablation experiment name\n",
    "    experiment_mode=experiment_mode)\n",
    "\n",
    "for (mkey, mvalue), (skey, svalue) in zip(mean_result.items(), stdev_result.items()):\n",
    "    print(f\"{mkey, skey}: {mvalue} ({svalue})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d4150-0baf-4264-8819-19bc61a74ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9550e8-7d69-4c8e-8355-8d6acc5bcdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd33f1-7f1d-4d76-82d9-ec56bea01849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
